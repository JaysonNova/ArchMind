# AI 服务层详细设计文档

## 文档版本信息

* **版本：** v1.0
* **创建日期：** 2026-02-01
* **最后更新：** 2026-02-01
* **文档状态：** Draft
* **基于文档：** 架构设计文档 v1.0

---

## 1. 文档概述

### 1.1 文档目的

本文档详细描述 ArchMind AI 多模型服务层的实现规范，包括:
- 统一模型适配器接口
- 多个 AI 模型适配器实现
- 模型管理器设计
- 智能��由策略
- 成本估算和监控

### 1.2 目标读者

- 后端开发工程师
- AI/ML 工程师
- 系统架构师

### 1.3 支持的 AI 模型

| 模型 | 提供商 | 用途 | 成本 |
|------|--------|------|------|
| Claude 3.5 Sonnet | Anthropic | PRD 生成 | $3/$15 per 1M tokens |
| GPT-4o | OpenAI | 通用场景 | $2.5/$10 per 1M tokens |
| Gemini 1.5 Pro | Google | 大文档处理 | $1.25/$5 per 1M tokens |
| Qwen-Max | 阿里云 | 中文内容 | ¥0.02/¥0.06 per 1K tokens |
| Wenxin 4.0 | 百度 | 中文内容 | ¥0.12 per 1K tokens |
| DeepSeek-V2 | DeepSeek | 成本敏感 | ¥0.001/¥0.002 per 1K tokens |
| Ollama | 本地 | 隐私模式 | 免费 |

---

## 2. 统一接口设计

### 2.1 AIModelAdapter 接口

**文件位置:** `lib/ai/types.ts`

```typescript
export interface AIModelAdapter {
  // 基础信息
  name: string;
  provider: string;
  modelId: string;

  // 核心能力
  generateText(prompt: string, options?: GenerateOptions): Promise<string>;
  generateStream(prompt: string, options?: GenerateOptions): AsyncIterator<string>;
  generateStructured<T>(prompt: string, schema: any): Promise<T>;

  // 模型配置
  getCapabilities(): ModelCapabilities;
  estimateCost(tokens: number): CostEstimate;
  isAvailable(): Promise<boolean>;
}

export interface GenerateOptions {
  temperature?: number;
  maxTokens?: number;
  topP?: number;
  stopSequences?: string[];
  systemPrompt?: string;
}

export interface ModelCapabilities {
  supportsStreaming: boolean;
  supportsStructuredOutput: boolean;
  supportsVision: boolean;
  maxContextLength: number;
  supportedLanguages: string[];
}

export interface CostEstimate {
  inputCost: number;
  outputCost: number;
  currency: string;
}

export enum TaskType {
  PRD_GENERATION = 'prd_generation',
  CHINESE_CONTENT = 'chinese_content',
  LARGE_DOCUMENT = 'large_document',
  COST_SENSITIVE = 'cost_sensitive',
  PRIVACY_MODE = 'privacy_mode',
}
```

---

## 3. 基础适配器实现

### 3.1 BaseAdapter 抽象类

**文件位置:** `lib/ai/adapters/base-adapter.ts`

```typescript
import type { AIModelAdapter, GenerateOptions, ModelCapabilities, CostEstimate } from '../types';

export abstract class BaseAdapter implements AIModelAdapter {
  abstract name: string;
  abstract provider: string;
  abstract modelId: string;

  abstract generateText(prompt: string, options?: GenerateOptions): Promise<string>;
  abstract generateStream(prompt: string, options?: GenerateOptions): AsyncIterator<string>;
  abstract getCapabilities(): ModelCapabilities;
  abstract estimateCost(tokens: number): CostEstimate;
  abstract isAvailable(): Promise<boolean>;

  async generateStructured<T>(prompt: string, schema: any): Promise<T> {
    const enhancedPrompt = `${prompt}\n\nPlease respond with valid JSON matching this schema:\n${JSON.stringify(schema, null, 2)}`;
    const text = await this.generateText(enhancedPrompt);

    // 提取 JSON
    const jsonMatch = text.match(/```json\n([\s\S]*?)\n```/) || text.match(/\{[\s\S]*\}/);
    const jsonStr = jsonMatch ? (jsonMatch[1] || jsonMatch[0]) : text;

    return JSON.parse(jsonStr) as T;
  }

  protected buildMessages(prompt: string, systemPrompt?: string): any[] {
    const messages: any[] = [];

    if (systemPrompt) {
      messages.push({ role: 'system', content: systemPrompt });
    }

    messages.push({ role: 'user', content: prompt });

    return messages;
  }
}
```

## 4. Claude 适配器实现

**文件位置:** `lib/ai/adapters/claude-adapter.ts`

```typescript
import Anthropic from '@anthropic-ai/sdk';
import { BaseAdapter } from './base-adapter';
import type { GenerateOptions, ModelCapabilities, CostEstimate } from '../types';

export class ClaudeAdapter extends BaseAdapter {
  name = 'Claude 3.5 Sonnet';
  provider = 'Anthropic';
  modelId = 'claude-3.5-sonnet';

  private client: Anthropic;

  constructor(apiKey: string) {
    super();
    this.client = new Anthropic({ apiKey });
  }

  async generateText(prompt: string, options?: GenerateOptions): Promise<string> {
    const response = await this.client.messages.create({
      model: 'claude-3-5-sonnet-20241022',
      max_tokens: options?.maxTokens || 4096,
      temperature: options?.temperature || 0.7,
      messages: [{ role: 'user', content: prompt }],
      system: options?.systemPrompt,
    });

    return response.content[0].type === 'text' ? response.content[0].text : '';
  }

  async *generateStream(prompt: string, options?: GenerateOptions): AsyncIterator<string> {
    const stream = await this.client.messages.stream({
      model: 'claude-3-5-sonnet-20241022',
      max_tokens: options?.maxTokens || 4096,
      temperature: options?.temperature || 0.7,
      messages: [{ role: 'user', content: prompt }],
      system: options?.systemPrompt,
    });

    for await (const chunk of stream) {
      if (chunk.type === 'content_block_delta' && chunk.delta.type === 'text_delta') {
        yield chunk.delta.text;
      }
    }
  }

  getCapabilities(): ModelCapabilities {
    return {
      supportsStreaming: true,
      supportsStructuredOutput: true,
      supportsVision: true,
      maxContextLength: 200000,
      supportedLanguages: ['en', 'zh', 'ja', 'es', 'fr', 'de'],
    };
  }

  estimateCost(tokens: number): CostEstimate {
    return {
      inputCost: (tokens / 1000000) * 3.0,
      outputCost: (tokens / 1000000) * 15.0,
      currency: 'USD',
    };
  }

  async isAvailable(): Promise<boolean> {
    try {
      await this.client.messages.create({
        model: 'claude-3-5-sonnet-20241022',
        max_tokens: 10,
        messages: [{ role: 'user', content: 'test' }],
      });
      return true;
    } catch {
      return false;
    }
  }
}
```

---

## 5. OpenAI 适配器实现

**文件位置:** `lib/ai/adapters/openai-adapter.ts`

```typescript
import OpenAI from 'openai';
import { BaseAdapter } from './base-adapter';
import type { GenerateOptions, ModelCapabilities, CostEstimate } from '../types';

export class OpenAIAdapter extends BaseAdapter {
  name = 'GPT-4o';
  provider = 'OpenAI';
  modelId = 'gpt-4o';

  private client: OpenAI;

  constructor(apiKey: string) {
    super();
    this.client = new OpenAI({ apiKey });
  }

  async generateText(prompt: string, options?: GenerateOptions): Promise<string> {
    const response = await this.client.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        ...(options?.systemPrompt ? [{ role: 'system' as const, content: options.systemPrompt }] : []),
        { role: 'user' as const, content: prompt }
      ],
      temperature: options?.temperature || 0.7,
      max_tokens: options?.maxTokens || 4096,
    });

    return response.choices[0]?.message?.content || '';
  }

  async *generateStream(prompt: string, options?: GenerateOptions): AsyncIterator<string> {
    const stream = await this.client.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        ...(options?.systemPrompt ? [{ role: 'system' as const, content: options.systemPrompt }] : []),
        { role: 'user' as const, content: prompt }
      ],
      temperature: options?.temperature || 0.7,
      max_tokens: options?.maxTokens || 4096,
      stream: true,
    });

    for await (const chunk of stream) {
      const content = chunk.choices[0]?.delta?.content;
      if (content) {
        yield content;
      }
    }
  }

  getCapabilities(): ModelCapabilities {
    return {
      supportsStreaming: true,
      supportsStructuredOutput: true,
      supportsVision: true,
      maxContextLength: 128000,
      supportedLanguages: ['en', 'zh', 'ja', 'es', 'fr', 'de'],
    };
  }

  estimateCost(tokens: number): CostEstimate {
    return {
      inputCost: (tokens / 1000000) * 2.5,
      outputCost: (tokens / 1000000) * 10.0,
      currency: 'USD',
    };
  }

  async isAvailable(): Promise<boolean> {
    try {
      await this.client.chat.completions.create({
        model: 'gpt-4o',
        messages: [{ role: 'user', content: 'test' }],
        max_tokens: 5,
      });
      return true;
    } catch {
      return false;
    }
  }
}
```

## 6. 模型管理器实现

**文件位置:** `lib/ai/model-manager.ts`

```typescript
import type { AIModelAdapter, TaskType } from './types';
import { ClaudeAdapter } from './adapters/claude-adapter';
import { OpenAIAdapter } from './adapters/openai-adapter';

export interface ModelConfig {
  default: string;
  fallback: string[];
  preferences: Record<TaskType, string[]>;
}

export class ModelManager {
  private adapters: Map<string, AIModelAdapter>;
  private config: ModelConfig;

  constructor() {
    this.adapters = new Map();
    this.config = this.loadConfig();
    this.initializeAdapters();
  }

  private loadConfig(): ModelConfig {
    return {
      default: 'claude-3.5-sonnet',
      fallback: ['gpt-4o', 'qwen-max'],
      preferences: {
        prd_generation: ['claude-3.5-sonnet', 'gpt-4o'],
        chinese_content: ['qwen-max', 'wenxin-4.0'],
        large_document: ['gemini-1.5-pro', 'claude-3.5-sonnet'],
        cost_sensitive: ['deepseek-v2', 'qwen-max'],
        privacy_mode: ['ollama-llama3'],
      },
    };
  }

  private initializeAdapters(): void {
    // Claude
    if (process.env.ANTHROPIC_API_KEY) {
      this.registerAdapter(new ClaudeAdapter(process.env.ANTHROPIC_API_KEY));
    }

    // OpenAI
    if (process.env.OPENAI_API_KEY) {
      this.registerAdapter(new OpenAIAdapter(process.env.OPENAI_API_KEY));
    }

    // 其他模型适配器...
  }

  registerAdapter(adapter: AIModelAdapter): void {
    this.adapters.set(adapter.modelId, adapter);
  }

  getAdapter(modelId: string): AIModelAdapter | undefined {
    return this.adapters.get(modelId);
  }

  async selectModel(taskType: TaskType): Promise<AIModelAdapter> {
    const preferences = this.config.preferences[taskType];

    // 尝试首选模型
    for (const modelId of preferences) {
      const adapter = this.adapters.get(modelId);
      if (adapter && await adapter.isAvailable()) {
        return adapter;
      }
    }

    // 降级到备用模型
    for (const modelId of this.config.fallback) {
      const adapter = this.adapters.get(modelId);
      if (adapter && await adapter.isAvailable()) {
        return adapter;
      }
    }

    throw new Error('No available model found');
  }

  async getAvailableModels(): Promise<AIModelAdapter[]> {
    const available: AIModelAdapter[] = [];
    for (const adapter of this.adapters.values()) {
      if (await adapter.isAvailable()) {
        available.push(adapter);
      }
    }
    return available;
  }
}
```

---

## 7. 使用示例

### 7.1 基本使用

```typescript
import { ModelManager } from '@/lib/ai/model-manager';
import { TaskType } from '@/lib/ai/types';

const manager = new ModelManager();

// 选择模型
const model = await manager.selectModel(TaskType.PRD_GENERATION);

// 生成文本
const text = await model.generateText('生成一个用户登录功能的 PRD', {
  temperature: 0.7,
  maxTokens: 4000,
});

console.log(text);
```

### 7.2 流式生成

```typescript
const model = await manager.selectModel(TaskType.PRD_GENERATION);

const stream = model.generateStream('生成一个用户登录功能的 PRD');

for await (const chunk of stream) {
  process.stdout.write(chunk);
}
```

---

## 8. 总结

本文档详细描述了 ArchMind AI 多模型服务层的完整实现规范，包括:

✅ **统一接口设计** - AIModelAdapter 接口
✅ **基础适配器** - BaseAdapter 抽象类
✅ **Claude 适配器** - Anthropic API 集成
✅ **OpenAI 适配器** - OpenAI API 集成
✅ **模型管理器** - 智能路由和降级策略
✅ **使用示例** - 实际代码示例

**关键设计决策:**
- 统一接口支持多种 AI 模型
- 基于任务类型的智能路由
- 自动降级机制确保可用性
- 流式输出支持实时反馈
- 成本估算帮助控制开支

**下一步:**
- 实现 PRD 生成引擎详细设计
- 实现 API 层详细设计
- 实现前端架构详细设计

