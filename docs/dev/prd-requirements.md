# ArchMind AI 产品需求文档 

## 1. 项目概述

* **产品名称：** ArchMind AI
* **项目定位：** 本地运行的 MVP 版本，个人产品开发辅助工具
* **愿景：** 让每一份历史文档都成为新功能的基石，消除产品迭代中的逻辑断层。
* **核心目标：** 通过"知识库 + 逻辑推理 + 自动化渲染"三位一体，实现从**想法**到**准交付物（PRD+Prototype）**的分钟级转化。
* **MVP 范围：** 专注于核心 RAG 功能和基础 PRD 生成，在本地环境验证技术可行性和用户价值。

---

## 2. MVP 目标与验证假设

### 2.1 MVP 用户定位

**当前阶段用户：** 开发者本人（全栈 AI 产品开发者）

**使用场景：**
* 个人产品开发过程中的需求文档管理
* 快速生成和迭代 PRD 文档
* 验证 RAG + LLM 技术在产品文档生成中的可行性
* 积累历史文档，建立个人产品知识库

**MVP 验证目标：**
* 技术可行性：RAG 检索 + 多模型 AI 服务能否生成高质量PRD
* 用户价值：是否真的能提升文档编写效率
* 架构合理性：本地运行的技术架构是否可扩展

### 2.2 核心价值假设

* **假设 1：** 历史 PRD 文档能够有效辅助新功能的需求编写
* **假设 2：** AI 生成的 PRD 质量能达到可用水平（需要少量修改即可使用）
* **假设 3：** 本地运行的 RAG 系统性能足够支撑实际使用
* **假设 4：** 单人开发可以在 3-4 个月内完成可用的 MVP

---

## 3. 技术方案对比（MVP 阶段）

**现有方案的局限性：**

| 方案 | 局限性 | ArchMind AI 的改进 |
| --- | --- | --- |
| **ChatGPT 直接生成** | 不了解历史产品逻辑，每次都需要重新描述背景 | 基于 RAG 的历史文档检索，自动理解产品上下文 |
| **Notion AI** | 无法学习历史文档，生成内容缺乏一致性 | 深度索引历史 PRD，确保逻辑一致性 |
| **手工编写 PRD** | 耗时长，容易遗漏边界情况 | AI 辅助生成 + 逻辑补全，提升效率和质量 |

**MVP 核心差异化：**
* 本地运行，数据完全私有
* 基于历史文档的上下文理解
* 专注于 PRD 生成的垂直场景

---

## 4. 用户痛点与解决方案

| 用户痛点 | ArchMind AI 解决方案 |
| --- | --- |
| **背景对齐难：** AI 不懂现有产品的复杂鉴权和业务逻辑。 | **RAG 增强：** 深度索引历史 PRD，确保新功能不脱离现有框架。 |
| **设计不统一：** 生成的原型风格与现有 App 完全不符。 | **视觉指纹提取：** 自动识别历史 UI 截图中的组件特征（圆角、色彩、间距）。 |
| **逻辑漏洞：** 手写 PRD 容易漏掉异常流程和边界条件。 | **逻辑补完算子：** 基于历史业务深度，自动补齐失败路径、空状态等描述。 |

---

## 3. 核心功能模块

### 3.1 资产中心 (The Core)

* **数据摄入：** 支持拖拽上传 PRD (Word/Markdown)、流程图 (Visio/Draw.io) 及 UI 截图。
* **语义链接：** 系统会自动建立“功能-页面-逻辑”的关联矩阵。
* **一致性评分：** 使用公式计算新需求与现有逻辑的兼容度：

```
一致性得分 = Σ(w_i × m_i) / N
```

*(其中 w_i 为逻辑权重，m_i 为匹配度，N 为关联点总数。评分低于 0.6 时系统会触发逻辑冲突预警。)*

### 3.2 自动化原型工厂 (Prototype Factory)

* **组件匹配渲染：** 读取历史 UI 中的设计系统 (Design System)，生成的线框图直接应用现有的按钮样式和布局比例。
* **低代码预览：** 生成基于 SVG 或 React 的交互原型，支持点击跳转。
* **Figma 对接：** 提供一键导出至 Figma 的功能，保持图层可编辑。

### 3.3 自动化 PRD 引擎 (PRD Studio)

* **智能章节填充：** 自动生成业务背景、流程图（Mermaid 语法）、功能详述、非功能需求。
* **变更影响分析 (Impact Analysis)：** 自动罗列出受新功能影响的旧有模块名单。
* **埋点与监控建议：** 基于历史数据偏好，自动建议新功能需要追踪的核心指标 (KPI)。

---

## 4. 业务流程 (User Flow)

1. **训练阶段：** 用户上传《2024年支付中心优化文档.docx》和 5 张结算页截图。
2. **输入需求：** 用户输入：“我想在结算页增加一个‘积分抵现’功能，需要支持部分抵扣。”
3. **逻辑检索：** AI 检索到该项目已有的“优惠券”和“余额”逻辑，判断积分抵现应遵循相似的扣减优先级。
4. **预览生成：**
* **左屏：** 显示修改后的结算页原型（带积分开关）。
* **右屏：** 显示新增的 PRD 章节，包括“积分扣减失败回滚逻辑”。


5. **导出：** 用户一键同步至飞书文档或 Figma。

---

## 7. MVP 验证指标

### 7.1 技术可行性验证

| 指标类型 | 具体指标 | 目标值 | 验证方式 |
| --- | --- | --- | --- |
| **RAG 检索准确率** | 检索到相关历史文档的准确率 | > 70% | 人工评估检索结果的相关性 |
| **PRD 生成质量** | 生成的 PRD 可用性评分 | > 7/10 | 自我评估生成内容的质量 |
| **响应速度** | 从输入到生成初稿的时间 | < 30 秒 | 系统计时 |
| **本地运行稳定性** | 连续运行无崩溃时间 | > 2 小时 | 实际使用测试 |

### 7.2 用户价值验证

* **效率提升：** PRD 编写时间是否缩短 50% 以上
* **内容采纳率：** 生成内容直接使用或轻微修改的比例 > 60%
* **逻辑完整性：** 生成的 PRD 是否包含必要的章节和逻辑流程
* **使用频率：** 是否愿意持续使用（每周至少使用 2 次）

### 7.3 系统性能指标

* **内存占用：** 本地运行时内存占用 < 2GB
* **文档处理速度：** 单个文档向量化时间 < 10 秒
* **数据库查询速度：** 向量检索响应时间 < 1 秒
* **并发处理能力：** 支持同时处理 3+ 个文档上传

---

## 8. 技术架构设计（单人 MVP 版本）

### 8.1 本地运行架构图

```
┌─────────────────────────────────────────────────────────┐
│                 前端层 (Frontend)                         │
│              Next.js + React + TypeScript                │
│           (使用 Shadcn UI 或 Ant Design)                 │
└─────────────────┬───────────────────────────────────────┘
                  │
┌─────────────────▼───────────────────────────────────────┐
│                API 层 (Next.js API Routes)               │
│              认证 + 业务逻辑 + AI 调用                    │
└─────────────────┬───────────────────────────────────────┘
                  │
        ┌─────────┴─────────┐
        │                   │
┌───────▼────────┐  ┌──────▼──────────────────────────┐
│  本地数据层     │  │  AI 服务层 (多模型支持)          │
│  (SQLite)      │  │  统一接口 + 多模型适配器          │
├────────────────┤  ├─────────────────────────────────┤
│ • 文档存储     │  │ • Claude (Anthropic)            │
│ • 向量存储     │  │ • GPT-4/GPT-4o (OpenAI)         │
│ • 本地文件     │  │ • Gemini (Google)               │
│                │  │ • 通义千问 (阿里云)              │
│                │  │ • 文心一言 (百度)                │
│                │  │ • DeepSeek                      │
│                │  │ • 本地模型 (Ollama)              │
└────────────────┘  └─────────────────────────────────┘
```

**本地架构优势：**
- **All-in-One**: Next.js 提供前后端一体化开发
- **快速部署**: Vercel 一键部署，零配置
- **成本低**: Supabase 免费套餐足够 MVP 使用
- **开发效率**: 使用成熟组件库，减少 UI 开发时间

### 8.2 本地运行技术栈（MVP 版本）

| 层级 | 技术选型 | 选择理由 |
| --- | --- | --- |
| **全栈框架** | Next.js 14 (App Router) | 前后端一体，部署简单，生态成熟 |
| **UI 框架** | Shadcn UI + Tailwind CSS | 现代化组件，易于定制 |
| **数据库** | SQLite + sqlite-vss | 轻量级本地数据库，支持向量检索 |
| **AI 引擎** | 多模型支持（统一接口） | 支持 Claude、GPT-4、Gemini、国产大模型等 |
| **向量检索** | sqlite-vss | 本地向量检索，无需额外服务 |
| **文件存储** | 本地文件系统 | 直接存储在项目目录，简单可靠 |
| **运行环境** | 本地开发环境 | npm run dev 本地运行，无需部署 |

### 8.3 简化关键技术模块

**1. RAG 检索引擎（本地版）**
- **文档处理：** 使用 LangChain 的文档加载器
- **向量化：** 使用 OpenAI text-embedding-3-small（成本低）
- **检索策略：** 简单的向量相似度检索（sqlite-vss）
- **实现工具：** LangChain.js + SQLite

**2. PRD 生成引擎（核心功能）**
- **多模型适配器：** 统一的 AI 服务接口，支持多种大模型
- **Prompt 工程：** 精心设计的 PRD 生成提示词，适配不同模型特性
- **结构化输出：** 支持各模型的 JSON mode 或结构化输出能力
- **流式输出：** 实时显示生成进度
- **模型切换：** 用户可根据需求选择不同的 AI 模型
- **实现工具：** 统一 SDK 封装 + 自定义 Prompt 模板

**3. UI 识别模块（Phase 2，可选）**
- **多模型视觉能力：** 支持 Claude Vision、GPT-4V、Gemini Vision 等
- **简化功能：** 仅提取颜色、布局等基础信息
- **降级方案：** 用户手动输入设计规范

### 8.4 本地数据安全（MVP 版本）

**数据存储：**
* **完全本地：** 所有文档和向量数据存储在本地 SQLite 数据库
* **文件权限：** 使用操作系统的文件权限管理
* **无云端存储：** 除 AI 模型 API 调用外，无数据上传到云端

**API 调用隐私：**
* **多模型支持：** 支持 Claude、GPT-4、Gemini、国产大模型等，用户可自由选择
* **数据传输：** 仅发送必要的文档内容用于生成，遵循各服务商的隐私政策
* **本地模型选项：** 支持 Ollama 等本地模型，实现完全离线运行
* **API Key 管理：** 使用环境变量存储多个 API Key，不提交到代码仓库
* **数据控制：** 用户完全控制使用哪个模型以及哪些文档发送给 AI 处理

### 8.5 多模型 AI 服务架构设计

#### 8.5.1 统一接口层设计

**核心设计原则：**
* **统一抽象：** 定义标准的 AI 服务接口，屏蔽不同模型的差异
* **插件化架构：** 每个模型作为独立的适配器插件
* **配置驱动：** 通过配置文件管理模型参数和切换策略

**接口定义示例：**
```typescript
interface AIModelAdapter {
  // 基础信息
  name: string;
  provider: string;

  // 核心能力
  generateText(prompt: string, options?: GenerateOptions): Promise<string>;
  generateStream(prompt: string, options?: GenerateOptions): AsyncIterator<string>;
  generateStructured(prompt: string, schema: JSONSchema): Promise<object>;

  // 多模态能力（可选）
  analyzeImage?(image: Buffer, prompt: string): Promise<string>;

  // 模型配置
  getCapabilities(): ModelCapabilities;
  estimateCost(tokens: number): number;
}
```

#### 8.5.2 支持的模型列表

| 模型提供商 | 模型名称 | 优势 | 适用场景 | 成本 |
| --- | --- | --- | --- | --- |
| **Anthropic** | Claude 3.5 Sonnet | 强大的文档理解和推理能力 | PRD 生成、逻辑分析 | 中 |
| **OpenAI** | GPT-4o / GPT-4 Turbo | 综合能力强，生态成熟 | 通用文本生成 | 中高 |
| **Google** | Gemini 1.5 Pro | 超长上下文，多模态能力强 | 大文档处理 | 中 |
| **阿里云** | 通义千问 Qwen-Max | 中文理解优秀，成本低 | 中文 PRD 生成 | 低 |
| **百度** | 文心一言 4.0 | 中文能力强，国内访问快 | 中文场景 | 低 |
| **DeepSeek** | DeepSeek-V2 | 性价比高，推理能力强 | 成本敏感场景 | 极低 |
| **本地模型** | Ollama (Llama 3, Qwen) | 完全离线，数据私密 | 隐私敏感场景 | 免费 |

#### 8.5.3 模型选择策略

**智能路由机制：**
* **任务类型匹配：** 根据任务类型自动推荐最适合的模型
  - PRD 生成：优先 Claude 3.5 Sonnet（推理能力强）
  - 中文文档：优先通义千问、文心一言（中文理解好）
  - 大文档处理：优先 Gemini 1.5 Pro（长上下文）
  - 成本敏感：优先 DeepSeek（性价比高）

* **降级策略：** 当首选模型不可用时，自动切换到备用模型
* **负载均衡：** 支持多个 API Key 轮询，避免单一限流

**用户配置选项：**
```yaml
ai_models:
  default: claude-3.5-sonnet
  fallback: [gpt-4o, qwen-max]

  preferences:
    prd_generation: claude-3.5-sonnet
    chinese_content: qwen-max
    cost_sensitive: deepseek-v2
    privacy_mode: ollama-llama3
```

---

## 9. 关键技术要求 (Non-functional)

* **多模态对齐：** 视觉模型识别出的 UI 组件名称必须与 PRD 中的术语对齐（例如：图中识别为“输入框”，文档中对应“搜索栏”）。
* **私有化部署支持：** 考虑到企业资产敏感性，支持本地向量库部署。
* **逻辑严密性：** 对涉及资金、权限的逻辑变更，系统必须强制生成“边界条件处理”章节。

---